{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning com scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial organizado por Danilo J. S. Bellini, realizado no dia 2018-03-31 com carga horária de 6 horas e 50 minutos, durante a *Python Sudeste 2018*. Este material foi baseado [neste mini-curso de scikit-learn](../2017-10-26_scikit-learn/mini-curso_scikit-learn.ipynb) realizado pelo mesmo autor em 2017-10-26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição [no SpeakerFight](https://speakerfight.com/events/python-sudeste-2018-tutoriais/#machine-learning-com-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial prático de machine learning visando apresentar conceitos básicos da área, convenções do scikit-learn e casos de uso.\n",
    "\n",
    "**Duração do tutorial**: de 6 a 8 horas\n",
    "\n",
    "**Conhecimento prévio necessário**: Familiaridade com o Python.\n",
    "\n",
    "Conhecer o stack científico do Python, principalmente numpy e matplotlib, facilitará a compreensão.\n",
    "\n",
    "**Pré-requisitos**:\n",
    "\n",
    "Trazer o computador (o tutorial é hands-on!) com o Python (preferencialmente o 3.6) e os seguintes pacotes instalados: numpy, scipy, matplotlib, seaborn, pandas, scikit-learn.\n",
    "\n",
    "O tutorial estará na forma de um Jupyter Notebook. O ideal é acompanhar o próprio notebook, mas tudo poderá ser feito por meio do editor de textos e do REPL que cada um preferir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicas gerais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se acabou o exercício, ajude o colega!\n",
    "* Sempre olhe as docstrings (`Shift + TAB` no Jupyter, `?` ao final no IPython/Jupyter, ou `help(...)` em qualquer REPL)\n",
    "* Não use \"copiar\" e \"colar\", a menos que você já tenha decorado e esteja seguro sobre o conteúdo, senão isso pode atrapalhar o aprendizado\n",
    "* Mantenha o foco no scikit-learn! Caso não entenda outras bibliotecas usadas além do scikit-learn (sobretudo `seaborn`, `matplotlib` e `scipy`), acredite no material e deixe para aprender sobre essa outra biblioteca em outro momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é o scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteca FLOSS (*Free/Libre and Open Source Software*, licença BSD) para Statistical/Machine Learning (data mining, data analysis), contendo recursos para:\n",
    "\n",
    "* **Aprendizado supervisionado**: classificação, regressão\n",
    "* **Aprendizado não-supervisionado**: *clustering*, estimativa de densidade\n",
    "* **Workflows estatísticos**: comparação, validação, escolha e combinação de parâmetros e modelos\n",
    "* **Pré-processamento**: redução de dimensionalidade, normalização, cálculo de *features*\n",
    "\n",
    "Além de bases de dados (datasets) e recursos auxiliares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](../2017-10-26_scikit-learn/ml_map.png)](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "Imagem obtida em: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os \"scikits\" são bibliotecas para processamento científico que podem ser vistas como add-ons para o SciPy: https://www.scipy.org/scikits.html\n",
    "\n",
    "Apesar do nome no PyPI seguir a convenção dos scikits, a importação do scikit-learn é realizada pelo nome `sklearn`. A API do scikit-learn está organizada por subpacotes, os quais costumam ser importados com `from sklearn import nome_do_subpacote` ou `from sklearn.nome_do_subpacote import nome_do_objeto`. A Documentação da API possui informações sobre todos esses subpacotes: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "Obviamente não veremos tudo neste tutorial, mas tentaremos entender as interfaces para utilizar os estimadores e outros recursos presentes no scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - O que é Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma área da ciência da computação que lida com algoritmos que extraem conhecimento (aprendem) a partir de dados fornecidos. Há, também, uma área da matemática chamada *modelagem estatística*, que tem um objetivo bastante similar: formalizar relações entre as variáveis \\[aleatórias\\] usando o formalismo matemático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parte mais relevante aqui é a intenção, a qual é sempre a mesma: obter valores/pesos/estatísticas a partir de um conjunto/massa/volume de dados (*dataset*) fornecido, e poder gerar algo útil para outros dados disponibilizados com a mesma estrutura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há diversas formas de se estruturar volumes de dados (e.g. árvores, grafos), mas, sobretudo quando falamos de utilizar o scikit-learn, lidamos com operações realizadas sobre dados tabulares numéricos da forma:\n",
    "\n",
    "|Feature $1$|Feature $2$|...|Feature $n$|Target|\n",
    "|--|--|--|--|--|\n",
    "|$X_{1,1}$|$X_{1,2}$|...|$X_{1,n}$|$y_1$|\n",
    "|...|...|...|...|\n",
    "|$X_{m,1}$|$X_{m,2}$|...|$X_{m,n}$|$y_m$|\n",
    "\n",
    "Cada linha é um exemplo/exemplar/ocorrência/item. As colunas possuem alguma informação/feição/característica/*feature* específica que é avaliada sobre todos os exemplos e que permite comparação entre linhas distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível representar a tabela com matrizes:\n",
    "\n",
    "|Features|Target|\n",
    "|--|--|\n",
    "|$\\mathbf{X}$|$\\mathbf{y}$|\n",
    "\n",
    "Em que $\\mathbf{X}$ é uma matriz $m \\times n$ (isto é, $m$ linhas/itens/objetos, e $n$ colunas/informações/*features*), e $\\mathbf{y}$ é uma matriz $m \\times 1$ (matriz coluna com o alvo/resultado). Como veremos a seguir, no aprendizado não-supervisionado essa coluna separada de alvo/*target* inexiste, mas no aprendizado supervisionado ela contém a \"saída\" associada à respectiva linha \"de entrada\" da matriz $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolação e truncamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda está tudo muito abstrato! Vamos começar com um exemplo de geometria analítica! Dado o ponto $(1, 5)$ e o ponto $(4, 11)$, qual seria a coordenada associada à abscissa $2$? Isto é, dada uma tabela da forma:\n",
    "\n",
    "|Abscissa (*feature*)|Coordenada (*target*)|\n",
    "|-|-|\n",
    "|$1$|$3$|\n",
    "|$4$|$9$|\n",
    "\n",
    "Temos a abscissa igual a $2$ como única \"entrada\"/*feature*, e queremos saber a coordenada (\"saída\") ainda desconhecida.\n",
    "\n",
    "|Abscissa (*feature*)|Coordenada (*target*)|\n",
    "|-|-|\n",
    "|$2$|*???*|\n",
    "\n",
    "Uma possível estratégia para encontrar essa coordenada seria traçar a reta que passa pelos dois pontos fornecidos, o que resultaria no valor $5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplots(nrows=1, ncols=1)[1]\n",
    "ax.set(xlabel=\"Abscissas\", ylabel=\"Coordenadas\", title=\"Interpolação linear\")\n",
    "ax.plot([1, 4], [3, 9], \"x--b\", ms=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra possível estratégia seria pegar o ponto com a abscissa mais próxima, que nesse caso seria o ponto $(1, 3)$, e usar sua ordenada como resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplots(nrows=1, ncols=1)[1]\n",
    "ax.set(xlabel=\"Abscissas\", ylabel=\"Coordenadas\", title=\"Vizinho mais próximo\")\n",
    "ax.plot([1, 2.5], [3, 3], \"--b\",\n",
    "        [2.5, 4], [9, 9], \"--b\",\n",
    "        [2.5, 2.5], [3, 9], \":r\",\n",
    "        [1, 4], [3, 9], \"xb\", ms=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E há inúmeras outras formas como podemos \"unir\" esses dois pontos! Esse exemplo é realmente mínimo, normalmente precisamos de mais informações sobre o contexto. Alguns exemplos com os mesmos dados que exigiriam algo mais sofisticado que o exposto até agora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O contradomínio pode ser limitado, extrapolar a reta obtida pela interpolação linear pode ultrapassar tais limites;\n",
    "\n",
    "|Horas de estudo na véspera da prova|Nota obtida (de 0 a 10)|\n",
    "|-|-|\n",
    "|$1$|$3$|\n",
    "|$4$|$9$|\n",
    "|$5$|*???*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podem existir pontos e assíntotas implícitas nas regras de negócio (e.g. o $(0, 0)$);\n",
    "\n",
    "|Quantidade de itens|Preço|\n",
    "|-|-|\n",
    "|$1$|$3$|\n",
    "|$4$|$9$|\n",
    "|$2$|*???*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As regras que regem o resultado podem ser completamente arbitrárias.\n",
    "\n",
    "|Número|Próxima potência de três|\n",
    "|-|-|\n",
    "|$1$|$3$|\n",
    "|$4$|$9$|\n",
    "|$3$|*???*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É inviável falar de machine learning nesses casos. Precisamos de mais dados!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda sem utilizar o scikit-learn, vamos criar uma classe para fazer a interpolação linear, escrita com uma interface similar à interface de estimadores do scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Preencher as lacunas marcadas com comentários `[...]` no código que segue, de maneira a garantir que o resultado impresso ao final é `[[5.]]` (um array bidimensional $1 \\times 1$ com um único elemento: o número 5).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinearInterpolator:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # [...]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return # [...]\n",
    "\n",
    "# Os dados que tínhamos!\n",
    "X_train = np.array([[1],\n",
    "                    [4]])\n",
    "y_train = np.array([[3],\n",
    "                    [9]])\n",
    "X_test = np.array([[2]])\n",
    "\n",
    "model = LinearInterpolator().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "```\n",
    "\n",
    "Observação: o `X` em maiúsculo viola a [PEP8](https://www.python.org/dev/peps/pep-0008/), mas esses nomes de variáveis são uma convenção do scikit-learn. Durante o exercício, apenas mantenha esses nomes, isso será explicado melhor mais adiante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que a equação reduzida da reta é:\n",
    "\n",
    "$$y = m x + q$$\n",
    "\n",
    "e que\n",
    "\n",
    "$$m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_1 - y_0}{x_1 - x_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solução](resposta-exercicio-interpolacao.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que, em uma prova de estatística descritiva, os alunos tiraram as seguintes notas (ignore a coluna de índices que o Pandas exibe, o relevante é apenas o par de colunas nominadas e o *scatterplot*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_hours = np.array([5.5, 8, 2.5, 7, 0, 1.25, 3.5,\n",
    "                    12, 9.5, 17, 4, 0.5, 14])[:, None]\n",
    "gradings = np.round(n_hours * .45 + 1 + np.random.randn(*n_hours.shape), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradings_df = pd.DataFrame([n_hours.flat, gradings.flat],\n",
    "                           index=[\"Horas de estudo\", \"Nota\"]).T\n",
    "gradings_df.plot.scatter(0, 1)\n",
    "gradings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É difícil enxergar a tendência linear apenas olhando números na tabela, o scatterplot deixa isso mais evidente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De qualquer forma, os dados não são reais, eles foram gerados a partir da reta:\n",
    "\n",
    "$$y_i = 0.45 X_{i,0} + 1 + \\varepsilon$$\n",
    "\n",
    "em que $\\varepsilon$ é um ruído normal padrão (média nula, variância unitária) somado a um ruído de arredondamento ($1$ casa decimal), e $y_i$ é a nota associada a $X_{i,0}$ horas de estudo. Conseguiríamos achar esses parâmetros $0.45$ e $1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo agora é estimar qual será a nota de um aluno que estudar por $11$ horas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução da regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que $X_{i,1} = 1$, isto é, há uma coluna adicional de entrada que é constante e igual a $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_X = gradings_df.assign(bias=1).drop(columns=[\"Nota\"]).values\n",
    "regr_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema passa a ser a obtenção do vetor $(w_0, w_1)$ de parâmetros que nos permita predizer a nota associada a $X_{i,0}$ horas de estudo:\n",
    "\n",
    "$$\\hat{y}_i = w_0 X_{i,0} + w_1 X_{i,1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matematicamente, a solução da regressão linear é o vetor de parâmetros $\\mathbf{X}^+ \\mathbf{y}$, em que $\\mathbf{X}^+$ é a pseudoinversa de $\\mathbf{X}$ (para uma prova matemática, veja a seção sobre regressão linear [deste link](https://github.com/danilobellini/scientific-literature/blob/master/1936-Fisher/Fisher_1936.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_w = np.linalg.pinv(regr_X) @ gradings_df[\"Nota\"].values\n",
    "regr_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São valores bastante próximos de $0.45$ e $1$, mas não idênticos. Com esses valores coletados a partir dos dados, para $11$ horas, o valor de $\\hat{y}$ é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_w.dot([11, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou $6.4$, se arredondarmos para uma casa decimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão linear com o scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de regressão linear por OLS (*ordinary least squares*, mínimos quadrados) acima exposto pode não ser matematicamente complicado de ser implementado diretamente com o Numpy e o Pandas, mas com o scikit-learn isso pode ser resolvido desta forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_model = linear_model.LinearRegression().fit(n_hours, gradings.flat)\n",
    "regr_model.predict([[11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas observações:\n",
    "\n",
    "- Chamamos o objeto `regr_model` de *estimador*\n",
    "- O `fit` recebe dados ditos \"de treinamento\", os quais são utilizados para obtermos estatísticas/pesos necessários para gerar predições\n",
    "- A coluna de valores fixos `1` (também chamada de *offset* ou viés/*bias*, responsável por tornar esse um modelo afim) não fez parte da entrada em instante nenhum: o próprio modelo já resolveu essa parte do problema (há um parâmetro de construção `fit_intercept` para controlar isso, ele por padrão é verdadeiro)\n",
    "- O `predict` serve para utilizar o estimador para estimar/predizer o resultado de alguma entrada fornecida, o que neste caso significa aplicar o modelo de regressão linear\n",
    "- Usar `regr_model.predict(11)` também funcionaria, mas manter sempre a convenção de utilizar o formato de uma matriz bidimensional $X$ \"de entrada\" evita potenciais ambiguidades\n",
    "- A entrada `y` do `fit` é um array unidimensional: se fosse bidimensional, funcionaria, mas faria o resultado também ser bidimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto para o `fit` como para o `predict`, pode-se utilizar dataframes do Pandas ao invés de arrays do Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.LinearRegression() \\\n",
    "            .fit(gradings_df.drop(columns=\"Nota\"),\n",
    "                 gradings_df[\"Nota\"]) \\\n",
    "            .predict(pd.DataFrame([11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo/estimador obtido possui as informações de $w_0$ e $w_1$, mas isso é específico dessa classe `linear_model.LinearRegression`, outras classes de estimadores não necessariamente possuem tal informação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(regr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_model.coef_[0], regr_model.intercept_ # Vetor de pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando o resultado desta regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_hours, gradings)\n",
    "plt.plot([-1, 19], regr_model.predict([[-1], [19]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há *datasets* disponíveis com o scikit-learn. Vamos pegar o *Boston House Prices dataset* e tentar estimar um modelo de regressão linear para estimar o preço de casas em Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os datasets que podem ser abertos diretamente com o scikit-learn são basicamente dicionários com pelo menos os seguintes valores associados às seguintes chaves:\n",
    "\n",
    "* `DESCR`: Descrição, em texto, do conteúdo do dataset\n",
    "* `data`: `np.ndarray` $2D$ com os *features* ou variáveis independentes (entrada), em que cada coluna é um *feature*\n",
    "* `target`: `np.ndarray` $1D$ com os alvos ou variáveis dependentes (saída)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando aplicável, há ainda:\n",
    "\n",
    "* `feature_names`: nomes dos *features* (colunas do `data`)\n",
    "* `target_names`: rótulos/*labels* para cada valor possível de `target` (modelos de classificação, ainda não vimos isso)\n",
    "* `images`: `np.ndarray` com a imagem de entrada (quando cada coluna/*feature* representa um pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As chaves desses objetos são, também, nomes de atributos (i.e., `boston[\"target\"]` e `boston.target` acessam o mesmo valor). Vejamos uma descrição do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)\n",
    "pd.DataFrame(boston.data, columns=boston.feature_names) \\\n",
    "  .assign(target=boston.target) \\\n",
    "  .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo do exercício: criar um modelo de regressão linear com as 400 primeiras linhas desse dataset, e calcular o erro quadrático total com as demais linhas do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra: É possível realizar diversas transformações nos dados. Por exemplo, o que acontece se a regressão for realizada sem a coluna `RAD`? E se esse índice for dividido em colunas `RAD=1`, `RAD=2`, etc. para cada valor possível da coluna, criando colunas numéricas em que os únicos valores possíveis para os dados são $0$ (falso) e $1$ (verdadeiro)? Esse processo chama-se *one-hot encoding*, e costuma ser utilizado para tratar variáveis categóricas. [Este link](http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/mvahtmlnode11.html) possui outras sugestões de transformações possíveis nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solução](resposta-exercicio-regressao.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3 - Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aprendizado supervisionado** é aquele em que nós possuímos a informação do resultado/alvo para as entradas (variáveis independentes) fornecidas. Aprendemos a partir de linhas da tabela, formada por pares `(vetor_de_features, alvo)`, em que o vetor de *features* de uma entrada está associado a um valor ou classe/rótulo `alvo` mensurado/fixado, que queremos ser capazes de predizer a partir de um novo vetor de *features*. Esse é o caso que acabamos de lidar ao falar sobre regressão, e também o que acontece com a classificação: há uma coluna de alvo/*target*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos dividir o aprendizado supervisionado de acordo com a natureza da variável alvo:\n",
    "\n",
    "* Classificação (variável dependente categórica): processo de identificação da classe-alvo a partir dos *features*\n",
    "* Regressão (variável dependente quantitativa): cálculo de um número a partir das variáveis de entrada, estando presente alguma noção de métrica/distância/similaridade entre os possíveis alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendo do contexto, há outros nomes possíveis para essa variáveis ([Dependent and independent variables na Wikipedia](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)):\n",
    "\n",
    "> [...], an independent variable is sometimes called a \"predictor variable\", \"regressor\", \"controlled variable\", \"manipulated variable\", \"explanatory variable\", \"exposure variable\" (see reliability theory), \"risk factor\" (see medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable\"\n",
    "\n",
    "> [...], a dependent variable is sometimes called a \"response variable\", \"regressand\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos abrir outro dataset famoso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict = datasets.load_iris()\n",
    "iris_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dict[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída categórica com índices para esses nomes\n",
    "iris_dict[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(iris_dict[\"target\"]) # 50 de cada classe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(iris_dict[\"data\"], columns=iris_dict[\"feature_names\"]) \\\n",
    "         .assign(species=[iris_dict[\"target_names\"][idx] for idx in iris_dict[\"target\"]])\n",
    "iris.iloc[[0, 50, 100], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriam esses dados linearmente separáveis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(iris, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há uma classe/espécie que é linearmente separável das demais! Você conseguiria colocar alguma reta acima em algum dos gráficos para separar uma distribuição das outras duas?\n",
    "\n",
    "As outras duas espécies criam um bloco só, parecem mais difíceis de separar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse dataset, na versão atual, contém erros de transcrição, felizmente irrelevantes para este tutorial. Caso haja interesse, veja o artigo *The use of multiple measurements in taxonomic problems* (1936) do R. A. Fisher e a análise em https://github.com/danilobellini/scientific-literature para mais informações sobre o Iris, discriminantes lineares e o fundamento estatístico de parte do que será visto a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificadores no scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scikit-learn, há um padrão para todos os **estimadores**, objetos que são treinados para estimar/predizer valores. No caso da regressão e da classificação, como já vimos, fazemos isso com os métodos `fit` e `predict`, um para treinar o estimador, o outro para utilizá-lo.\n",
    "\n",
    "```python\n",
    "from sklearn import módulo_com_estimador\n",
    "estimator = módulo_com_estimador.ClasseDoEstimador(*args, **kwargs)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "```\n",
    "\n",
    "Cada entrada pode ser tanto um *array* do NumPy como um *dataframe* do Pandas na estrutura de tabela já vista, mantendo os nomes `X` e `y`:\n",
    "\n",
    "* `X_train` e `X_test` são $2D$ e possuem um exemplo/exemplar/item por linha, uma informação/*feature* por coluna, é como as matrizes do campo `data` dos datasets que vêm com o scikit-learn\n",
    "* `y_train` é $1D$ e possui o resultado de cada respectivo evento, i.e., `y_train[i]` é o resultado do vetor de features `X_train[i]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: Os nomes `X_train` e `X_test` não seguem a [PEP8](https://www.python.org/dev/peps/pep-0008/). Eles decorrem do uso tradicional de letras maiúsculas para representar matrizes e letras minúsculas para representar vetores, em matemática. A variável `X` é tradicionalmente utilizada para as matrizes de \"entrada\" e a `y` para o vetor (matriz coluna) de \"saída\". Porém, isso é apenas uma convenção de nomes, o contexto pode sugerir nomes mais descritivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue uma lista não-exaustiva dos classificadores disponíveis no scikit-learn:\n",
    "\n",
    "* `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`\n",
    "* `sklearn.ensemble.AdaBoostClassifier`\n",
    "* `sklearn.ensemble.RandomForestClassifier`\n",
    "* `sklearn.gaussian_process.GaussianProcessClassifier`\n",
    "* `sklearn.linear_model.LogisticRegressionCV`\n",
    "* `sklearn.linear_model.Perceptron` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.naive_bayes.GaussianNB` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.neighbors.KNeighborsClassifier`\n",
    "* `sklearn.neural_network.MLPClassifier` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.svm.LinearSVC`\n",
    "* `sklearn.svm.SVC`\n",
    "* `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível identificar a espécie de Iris a partir das $4$ medidas (largura e comprimento das sépalas e pétalas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.iloc[:, :4]\n",
    "y = iris[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm # SVM = Support Vector Machine\n",
    "svm_estimator = svm.SVC(kernel=\"linear\") # Classificador linear\n",
    "svm_estimator.fit(X, y) # Treino!\n",
    "svm_estimator.predict(X) # Parece com 50 setosa, 50 versicolor, 50 virginica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a proporção de acertos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_estimator.score(X, y) # Realiza o predict internamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando temos um classificador, como podemos avaliá-lo?\n",
    "\n",
    "Poderíamos usar os dados que temos para realizar essa avaliação, porém se esses dados são os mesmos utilizados para treinar o estimador, talvez estejamos propagando um vício sem saber, obtendo um resultado demasiado otimista devido à falta de variabilidade. O nome dado a esse resultado é **overfitting**.\n",
    "\n",
    "Para auxiliar o entendimento do conceito, podemos fazer uma analogia disso com a aplicação de uma prova depois de uma aula. Durante a aula, o aluno pode encontrar um padrão como \"*as respostas desse tipo de exercício formam uma P.A.*\", por isso ter ocorrido nos exemplos mostrados, e achar que essa regra generalizava o que foi ensinado. Mas, se o padrão encontrado era meramente contingencial, esse aluno poderá se sair mal na prova por responder com os detalhes irrelevantes que memorizou ao invés de tentar aprender o conteúdo.\n",
    "\n",
    "Por outro lado, em uma aula o professor explicita o que é esse conteúdo, enquanto que em generalizações realizadas a partir de dados, qualquer padrão, mesmo que não seja considerado o mais simples por algum critério, pode ser o conteúdo, e o que queremos testar é se o padrão encontrado faz sentido como generalização dos dados e dos eventos futuros. Uma forma de testar isso é reservando uma parte dos dados que temos para validar/testar a generalização realizada a partir dos demais dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo o dataset em 2 partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma prática comum em *machine learning* é a de dividir os dados fornecidos em $2$ conjuntos: um para a realização do treinamento (`fit`) do estimador, outro para a realização de uma avaliação do estimador obtido. Um conjunto para o \"treino\", outro para \"testes\". Entre os diversos modelos, adota-se o que minimiza o erro olhando apenas para o conjunto de testes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](../2017-10-26_scikit-learn/overfitting.png)](https://en.wikipedia.org/wiki/File:Overfitting_svg.svg)\n",
    "\n",
    "(Imagem obtida no Wikipedia)\n",
    "\n",
    "As curvas do gráfico acima representam a taxa de erro quando avaliamos o modelo com o conjunto de testes (vermelho) ou com o conjunto de treino (azul). O eixo das abscissas representa os modelos, por exemplo os resultados de cada iteração de um processo de treinamento. Continuar um treinamento pode nos levar a uma contínua melhora na acurácia do modelo quando olhamos somente para o conjunto de treinamento, mas em algum instante o resultado aplicado ao conjunto de testes indica que o modelo começou a perder acurácia: overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y,\n",
    "                                     test_size=.25,\n",
    "                                     random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso separou $25\\%$ dos dados para teste e $75\\%$ para treino. Esse é o *default* do scikit-learn, o argumento nominado `test_size` não precisava ter sido passado nesse caso, mas é útil explicitar esse número, e torna prático mudá-lo, caso necessário.\n",
    "\n",
    "Poderíamos fazer esse particionamento manualmente, mas isso já particionou nossos dados embaralhando-os, sem perder a correspondência de índices entre as respectivas entradas e saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar o `fit` novamente sobreescreve os dados, apagando o treino anterior!\n",
    "# Classificadores que permitem aprendizado incremental possuem um método `partial_fit`!\n",
    "svm_estimator.fit(X_train, y_train)\n",
    "y_pred = svm_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos avaliar se esse resultado `y_pred` é similar ao `y_test`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos realizar avaliações sumarizando os erros e acertos de cada natureza em uma matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linhas representam os resultados coletados, as colunas representam os resultados estimados. Cuidado para não se confundir: isso é o transposto do [exposto no Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix), mas é a forma descrita na documentação do scikit-learn.\n",
    "\n",
    "O scikit-learn sempre ordena os dados ao apresentá-los, de maneira que as 3 linhas e as 3 colunas referem-se às espécies *setosa*, *versicolor* e *virginica* (ou 0, 1 e 2, se estivéssemos usando o `datasets.load_iris()[\"target\"]`), nesta ordem. Para fixar uma ordem qualquer, usa-se o argumento nominado `labels`.\n",
    "\n",
    "Colocando rótulos com o Pandas para melhorar a visualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_pred, labels=labels),\n",
    "             index=[[\"y_test\"] * 3, labels],\n",
    "             columns=[[\"y_pred\"] * 3, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre classificação, o que fizemos até o momento pode ser re-escrito da seguinte forma, sem utilizar o Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection, svm, metrics\n",
    "\n",
    "iris_dict = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(iris_dict[\"data\"], iris_dict[\"target\"],\n",
    "                                     test_size=.25, random_state=0)\n",
    "\n",
    "# Extra: o \"fit\" devolve o próprio objeto estimador\n",
    "svm_estimator = svm.SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "y_pred = svm_estimator.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_pred) # Implicitamente labels=[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem fixar o `random_state`, o resultado talvez não seja o mesmo, pois chamar `train_test_split` re-embaralha os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando as técnicas vistas, treinar um classificador para separar os seguintes blobs gerados aleatoriamente, avaliando o resultado obtido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob, y_blob = datasets.make_blobs(n_samples=2500, n_features=2, centers=5, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.FacetGrid(pd.DataFrame(X_blob).assign(y=y_blob), hue=\"y\", aspect=1, size=7) \\\n",
    "       .map(plt.scatter, 0, 1, s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solução para os dados fornecidos](resposta-exercicio-classificacao.ipynb)\n",
    "\n",
    "[Solução para n_samples=2000, centers=4, random_state=32](../2017-10-26_scikit-learn/resposta-exercicio-1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4 - Combinando blocos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o que foi feito até agora, é suficiente saber que o classificador `sklearn.svm.SVC(kernel=\"linear\")` e o `sklearn.svm.LinearSVC` encontram um discriminante linear para separar as classes, maximizando a distância com relação às amostras (margem larga), a depender do valor do `C` (quanto maior, mais tenderá à margem máxima).\n",
    "\n",
    "Porém, um discriminante linear é um hiperplano no espaço de *features* que consegue apenas separar duas classes (espécies, no caso do Iris dataset). Para funcionar com múltiplas classes, o `sklearn.svm.SVC` resolve isso agindo como um \"one-on-one classifier\". O que é isso? Tem alternativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas classes são *decorators* (no sentido do *design pattern*) para classificadores binários:\n",
    "\n",
    "* `sklearn.multiclass.OneVsOneClassifier`: Classifica todos os pares de classes separadamente, selecionando o que tiver o maior número de votos\n",
    "* `sklearn.multiclass.OneVsRestClassifier`: Cria um classificador para cada classe, avaliando se pertence à mesma ou ao conjunto de todas as demais classes\n",
    "\n",
    "No caso do SVM, se quiséssemos usá-lo como *one-vs-rest*, teríamos de instanciar o estimador desta maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import multiclass\n",
    "one_vs_rest_svm_estimator = multiclass.OneVsRestClassifier(svm.SVC(kernel=\"linear\"))\n",
    "one_vs_rest_svm_estimator.fit(X_train, y_train)\n",
    "one_vs_rest_svm_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos acesso aos classificadores binários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vs_rest_svm_estimator.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `decision_function` do estimador (classificador) binário é usado como critério de desempate, ou a primeira coluna do `predict_proba`, que representa a probabilidade da classe \"positiva\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No caso do SVM, estas são as distâncias com relação ao\n",
    "# hiperplano que separa as classes\n",
    "one_vs_rest_svm_estimator.estimators_[0].decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quando não é um classificador binário, cada coluna do\n",
    "# `decision_function` refere-se a um classificador binário\n",
    "one_vs_rest_svm_estimator.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais informações sobre classificadores multi-classe, veja http://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendo do estimador, um ajuste aditivo/multiplicativo para normalização da entrada pode ser fundamental para treiná-lo, influenciando o comportamento do mesmo. O fluxo do pré-processamento é similar ao de classificação, dessa vez com os métodos `fit` e `transform`:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "pre_processor = preprocessing.ClasseDePréProcessamento(*args, **kwargs)\n",
    "pre_processor.fit(X_train) # Encontra os parâmetros para pré-processar\n",
    "X_data_p = pre_processor.transform(X_data) # Processa um conjunto de dados\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando aplicado nos dados de treino, esse passo de transformação/pré-processamento passa a ser parte do pipeline de classificação, sendo necessário aplicá-lo nos dados antes do `predict` do classificador:\n",
    "\n",
    "```python\n",
    "X_train_p = pre_processor.transform(X_train)\n",
    "X_test_p = pre_processor.transform(X_test)\n",
    "\n",
    "estimator.fit(X_train_p, y_train) # Classificador do dado pré-processado!\n",
    "y_pred = estimator.predict(X_test_p) # Predição também exige dado pré-processado!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes comuns para pré-processar colunas/*features* independentemente incluem:\n",
    "\n",
    "* `StandardScaler`: subtrai a média e divide pelo desvio padrão\n",
    "* `RobustScaler`: subtrai a mediana e divide pelo IQR\n",
    "* `MinMaxScaler`: transformação afim do intervalo `[mínimo, máximo]` dos dados para o intervalo `[0, 1]`\n",
    "* `MaxAbsScaler`: transformação afim do intervalo `[-m, m]` para o intervalo `[-1, 1]`, em que `m` é o máximo do valor absoluto do *feature*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `MinMaxScaler`, o que muda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "iris_scaler = preprocessing.MinMaxScaler()\n",
    "iris_scaler.fit(X_train)\n",
    "X_train_p = iris_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_measurements = [cname.rsplit(None, 1)[0] for cname in iris_dict[\"feature_names\"]]\n",
    "iris_p = pd.DataFrame(X_train_p, columns=iris_measurements) \\\n",
    "           .assign(species=[iris_dict[\"target_names\"][idx] for idx in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_p, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exceção, a classe `sklearn.preprocessing.Normalizer` realiza um pré-processamento em cada linha, isoladamente, para esta ter norma unitária. Por não ter estado, o método `fit` de tal classe não faz nada, porém ele existe para que essa classe possa fazer parte de uma pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p_normalized = preprocessing.Normalizer().transform(X_train_p)\n",
    "iris_p_normalized = pd.DataFrame(X_train_p_normalized, columns=iris_measurements) \\\n",
    "                      .assign(species=[iris_dict[\"target_names\"][idx] for idx in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_p_normalized, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `Normalizer` realizou uma transformação não-linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um pouco burocrático ter de realizar o pré-processamento manualmente antes do classificador:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing, módulo_com_classificador\n",
    "\n",
    "pre_processor = preprocessing.ClasseDePréProcessamento(*args, **kwargs)\n",
    "pre_processor.fit(X_train)\n",
    "\n",
    "X_train_p = pre_processor.transform(X_train)\n",
    "classifier = módulo_com_classificador.ClasseDoClassificador(*args, **kwargs)\n",
    "classifier.fit(X_train_p, y_train)\n",
    "\n",
    "X_test_p = pre_processor.transform(X_test)\n",
    "y_pred = classifier.predict(X_test_p)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, queremos um objeto classificador que abstraia o processo inteiro de pré-processamento e de classificação, isto é, um classificador que já realiza o pré-processamento, para não ser necessário pré-processar cada entrada antes de usar o classificador. A realidade é que isso existe! O `sklearn.pipeline.Pipeline` conecta blocos de pré-processamento (objetos com `fit` e `transform`) a um único classificador recebendo uma lista de pares:\n",
    "\n",
    "```python\n",
    "from sklearn import pipeline\n",
    "pipeline_classifier = pipeline.Pipeline([\n",
    "    (\"Nome da etapa 1 de pré-processamento\", objeto_pre_processador1),\n",
    "    (\"Nome da etapa 2 de pré-processamento\", objeto_pre_processador2),\n",
    "    # ...\n",
    "    (\"Nome da etapa N de pré-processamento\", objeto_pre_processadorN),\n",
    "    (\"Nome da etapa de classificação\", objeto_classificador),\n",
    "])\n",
    "```\n",
    "\n",
    "ou, usando nomes automáticos para as etapas, com `sklearn.pipeline.make_pipeline`:\n",
    "\n",
    "```python\n",
    "pipeline_classifier = pipeline.make_pipeline(\n",
    "    objeto_pre_processador1,\n",
    "    objeto_pre_processador2,\n",
    "    # ...\n",
    "    objeto_pre_processadorN,\n",
    "    objeto_classificador,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "pipeline.Pipeline([(\"normaliza\", preprocessing.StandardScaler()),\n",
    "                   (\"classifica\", svm.LinearSVC())]) \\\n",
    "        .fit(X_train, y_train) \\\n",
    "        .score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: OCR de números!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn possui um dataset de dígitos escritos à mão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dict = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar os primeiros 10 dígitos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.subplots(2, 5, figsize=(15, 6), sharex=True, sharey=True)[1]\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits_dict[\"images\"][idx], cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parte 1*: Bisbilhotar/fuçar/investigar esse dataset. Nesse passo é preciso, no mínimo, identificar o tamanho de cada imagem, o número total de imagens e a quantidade de cada dígito presente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parte 2*: Utilizando algum dos classificadores disponíveis com o scikit-learn e as técnicas já vistas, criar um OCR para dígitos a partir desse dataset, reservando $30\\%$ do mesmo para a realização de uma avaliação exposta na forma de uma matriz de erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vídeo de lightning talk resolvendo o exercício em 9 minutos](https://youtu.be/kDmsYpRuNPA?t=45m50s)\n",
    "\n",
    "[Solução (material produzido na lightning talk)](../2017-11-13_OCR_GruPy-SP_10_anos.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5 - Aprendizado não-supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora estudamos apenas o processo de classificação supervisionada, no qual tínhamos um número ou um rótulo categórico associado a cada vetor de *features*, e gostaríamos de predizê-lo quando novos dados surgissem. Se tivéssemos apenas os *features*, poderíamos inferir alguma informação a partir dos mesmos? Sim, e esses são os processos de aprendizado não-supervisionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering com o k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente o algoritmo mais famoso de clustering é o k-means. Ele está implementado no scikit-learn na classe `sklearn.cluster.KMeans`. Como classificador, ele possui os métodos `fit` e `predict`, mas dessa vez o `fit` recebe apenas uma matriz de features para treinamento, e os labels são apenas números de $0$ a $k - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O k-means realiza um ciclo para a obtenção de \"centróides\":\n",
    "\n",
    "1. Define $k$ pontos como centróides\n",
    "2. Classifica cada ponto (linha da matriz de entrada) com o centróide mais próximo do mesmo (distância euclideana, tipicamente)\n",
    "3. Atualiza os centróides com os novos centróides de cada conjunto classificado\n",
    "\n",
    "A repetição dos passos $2$ e $3$ até a convergência é o que caracteriza o algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](../2017-10-26_scikit-learn/K-means_convergence.gif)](https://en.wikipedia.org/wiki/File:K-means_convergence.gif)\n",
    "\n",
    "(Imagem obtida no Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificar na prática, vamos aplicar o k-means aos blobs gerados anteriormente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O primeiro parâmetro é o número de clusters\n",
    "kmeans_model = cluster.KMeans(5, random_state=42)\n",
    "kmeans_model.fit(X_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_blob_pred = kmeans_model.predict(X_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Mesmo sem conhecer as classes de cada amostra, o algoritmo encontrou $5$ clusters a partir apenas dos pares de coordenadas de cada ponto. Vamos comparar visualmente o resultado desse processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob_dframe = pd.DataFrame(X_blob)\n",
    "full_blob_dframe = pd.concat([\n",
    "    X_blob_dframe.assign(y=y_blob, origin=\"Original\"),\n",
    "    X_blob_dframe.assign(y=y_blob_pred, origin=\"K-Means\")\n",
    "])\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.FacetGrid(full_blob_dframe, hue=\"y\", col=\"origin\",\n",
    "                  aspect=1, size=5, sharey=True) \\\n",
    "       .map(plt.scatter, 0, 1, s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecem bastante próximos! As cores diferentes apenas representam que o índice arbitrário de cada cluster não são iguais. Podemos fingir que esse processamento foi supervisionado e calcular uma matriz de erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casamento/reordenação das classes realizada empiricamente de forma a\n",
    "# fazer a diagonal da matriz ter os maiores valores de cada coluna\n",
    "metrics.confusion_matrix(y_blob, np.array([4, 3, 1, 0, 2])[y_blob_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas $58$ dos $2500$ pontos usados para gerar os blobs não coincidiram. Mesmo sendo uma técnica de aprendizado não-supervisionado, pode-se dizer que ela foi eficaz em identificar as classes nesse caso artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar o k-means ao Iris dataset e verificar o resultado através de uma matriz de erro. O k-means consegue classificar corretamente as $3$ espécies? E se usássemos outro valor para o $k$, como $2$, $4$ ou $6$, o que acontece?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solução](resposta-exercicio-clustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação de fontes de áudio com ICA - Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado um arquivo de áudio stereo, por exemplo:\n",
    "\n",
    "https://freesound.org/s/380165/\n",
    "\n",
    "Podemos separar os lados esquerdo e direito do áudio, convertendo em dois arquivos WAVE, `left.wav` e `right.wav`, usando o FFmpeg:\n",
    "\n",
    "```shell\n",
    "ffmpeg -i arquivo_baixado.mp3 -map_channel 0.0.0 left.wav -map_channel 0.0.1 right.wav\n",
    "```\n",
    "\n",
    "Os lados esquerdo e direito são muito similares nesse arquivo de áudio, mas seria possível separar as duas fontes sonoras (os $2$ cantores), ou pelo menos criar dois arquivos de áudio em que algum dos cantores está em maior evidência que o outro?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar encontrar componentes independentes no áudio usando o `sklearn.decomposition.FastICA`.\n",
    "\n",
    "Para ler os arquivos de áudio como um *array* do NumPy e armazenar depois os resultados em arquivos WAVE, vamos utilizar o módulo `scipy.io.wavfile` do SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "rate, left = wavfile.read(\"left.wav\")\n",
    "right = wavfile.read(\"right.wav\")[1]\n",
    "rate # Amostras de áudio por segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input = np.stack([left, right]).T\n",
    "audio_input.shape # Par (amostras por canal, número de canais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_input.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O FastICA no scikit-learn é um algoritmo de decomposição, o qual se comporta como uma rotina de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "ica = decomposition.FastICA(n_components=2, random_state=42)\n",
    "ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como outros modelos de pré-processamento, há um método `fit_transform` que unifica os dois passos `fit` e `transform` já vistos, e pode ser mais eficiente caso o processo do `fit` já exija o cálculo do resultado esperado pelo `transform`. A expressão:\n",
    "\n",
    "```python\n",
    "processor.fit_transform(X)\n",
    "```\n",
    "\n",
    "é funcionalmente equivalente a:\n",
    "\n",
    "```python\n",
    "processor.fit(X).transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = preprocessing.MaxAbsScaler().fit_transform(ica.fit_transform(audio_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile.write(\"result_0.wav\", rate, output[:, 0])\n",
    "wavfile.write(\"result_1.wav\", rate, output[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados desse processamento estão disponíveis nos links:\n",
    "\n",
    "* `result_0.wav`: https://freesound.org/s/406216/\n",
    "* `result_1.wav`: https://freesound.org/s/406215/\n",
    "\n",
    "As vozes não estão totalmente separadas, mas, por exemplo, a partir do instante 1:37 (últimos 20 segundos) é possível notar que cada áudio destaca mais uma das vozes, distintamente, sem distorcê-las.\n",
    "\n",
    "É possível obter resultados melhores usando técnicas específicas para essa finalidade, mas é importante notar que esse processo de segregação em componentes independentes não utilizou nada além do arquivo de áudio stereo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para maiores detalhes sobre ICA no scikit-learn, vejam estes dois exemplos na documentação do scikit-learn:\n",
    "\n",
    "* http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html\n",
    "* http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html\n",
    "\n",
    "O primeiro link contém, em particular, uma decomposição de sinais sintéticos, o qual ilustra bem o processo de decomposição realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte Final - O aprendizado não acaba aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muitas coisas importantes que não puderam ser apresentadas neste tutorial introdutório, desde questionamentos gerais de ciência de dados:\n",
    "\n",
    "* Como coletar os dados, e como esse processo pode influenciar os processos que seguem?\n",
    "* Como identificar quais dados são relevantes para um estimador?\n",
    "* Como tratar dados incompletos, espúrios e outliers?\n",
    "\n",
    "Como questionamentos mais diretamente vinculados ao scikit-learn:\n",
    "\n",
    "* Como selecinar o algoritmo de classificação?\n",
    "* Como podemos fazer para encontrar o valor ideal do `k`?\n",
    "* Como encontrar os melhores coeficientes do SVM?\n",
    "* Como tratar *features* categóricos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte desse conteúdo exige conhecimento não apenas do scikit-learn, mas com outras bibliotecas. Por exemplo, para tratar features categóricos pode-se usar o `sklearn.preprocessing.OneHotEncoder`, mas talvez o `pd.get_dummies` do Pandas seja mais adequado para o que se deseja fazer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Minha sugestão para o autodidata que deseja continuar aprendendo sobre scikit-learn é que procure por material sobre:\n",
    "\n",
    "* PCA: Decomposição fundamentada nas dimensões de maior variabilidade comumente utilizada para redução da dimensionalidade\n",
    "* Grid Search: busca exaustiva por parâmetros para os estimadores\n",
    "* Criação de classes personalizadas de estimadores e de transformadores, usadas em pipelines incluindo *feature union*\n",
    "\n",
    "E veja os exemplos disponibilizados em http://scikit-learn.org/stable/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FIM! Espero que tenha gostado! =D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
