{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-curso de scikit-learn @ SciPy-SP\n",
    "\n",
    "*Machine Learning em Python!*\n",
    "\n",
    "por Danilo J. S. Bellini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante**: Há uma versão mais recente e completa deste curso/tutorial de Machine Learning [neste link](../2018-03-31_scikit-learn/sklearn_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 0 - Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo do curso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Expor as convenções da biblioteca\n",
    "* Ilustrar alguns conceitos básicos de Machine Learning\n",
    "* Apresentar o vínculo com outras bibliotecas do Python\n",
    "* Hands on!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo gerenciador de pacotes de sua distribuição, `pip`, ou `conda`.\n",
    "Maiores informações em http://scikit-learn.org/stable/install.html\n",
    "\n",
    "Outros pacotes explicitamente utilizados neste mini-curso:\n",
    "\n",
    "* NumPy\n",
    "* SciPy\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "* Pandas\n",
    "* Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicas gerais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se acabou o exercício, ajude o colega!\n",
    "* Sempre olhe as docstrings (`Shift + TAB` no Jupyter, `?` ao final no IPython/Jupyter)\n",
    "* Não use \"copiar\" e \"colar\", a menos que você já tenha decorado e esteja seguro sobre o conteúdo, senão isso pode atrapalhar o aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é o scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteca FLOSS (BSD) para Statistical/Machine Learning (data mining, data analysis), contendo recursos para:\n",
    "\n",
    "* **Aprendizado supervisionado**: classificação, regressão\n",
    "* **Aprendizado não-supervisionado**: *clustering*, estimativa de densidade\n",
    "* **Workflows estatísticos**: comparação, validação, escolha e combinação de parâmetros e modelos\n",
    "* **Pré-processamento**: redução de dimensionalidade, normalização, cálculo de *features*\n",
    "\n",
    "Além de bases de dados (datasets) e diversos outros recursos auxiliares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](ml_map.png)](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "Imagem obtida em: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os \"scikits\" são bibliotecas para processamento científico que podem ser vistas como add-ons para o SciPy: https://www.scipy.org/scikits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar do nome no PyPI seguir a convenção dos scikits, a importação do scikit-learn é realizada pelo nome `sklearn`, e o conteúdo da biblioteca está organizado na forma de módulos e subpacotes:\n",
    "\n",
    "* `sklearn.base`\n",
    "* `sklearn.calibration`\n",
    "* `sklearn.cluster`\n",
    "* `sklearn.covariance`\n",
    "* `sklearn.cross_decomposition`\n",
    "* `sklearn.datasets`\n",
    "* `sklearn.decomposition`\n",
    "* `sklearn.discriminant_analysis`\n",
    "* `sklearn.dummy`\n",
    "* `sklearn.ensemble`\n",
    "* `sklearn.exceptions`\n",
    "* `sklearn.feature_extraction`\n",
    "* `sklearn.feature_selection`\n",
    "* `sklearn.gaussian_process`\n",
    "* `sklearn.isotonic`\n",
    "* `sklearn.kernel_approximation`\n",
    "* `sklearn.kernel_ridge`\n",
    "* `sklearn.linear_model`\n",
    "* `sklearn.manifold`\n",
    "* `sklearn.metrics`\n",
    "* `sklearn.mixture`\n",
    "* `sklearn.model_selection`\n",
    "* `sklearn.multiclass`\n",
    "* `sklearn.multioutput`\n",
    "* `sklearn.naive_bayes`\n",
    "* `sklearn.neighbors`\n",
    "* `sklearn.neural_network`\n",
    "* `sklearn.pipeline`\n",
    "* `sklearn.preprocessing`\n",
    "* `sklearn.random_projection`\n",
    "* `sklearn.semi_supervised`\n",
    "* `sklearn.svm`\n",
    "* `sklearn.tree`\n",
    "* `sklearn.utils`\n",
    "\n",
    "A Documentação da API possui informações sobre todos esses subpacotes: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "Obviamente não veremos tudo neste tutorial introdutório, mas tentaremos entender as interfaces para utilizar os estimadores e outros recursos presentes no scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Aprendizado supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizado supervisionado é aquele em que nós possuímos a informação do resultado/alvo para as entradas (variáveis independentes) fornecidas. Aprendemos a partir de pares `(vetor_de_features, alvo)`, em que o vetor de *features* de uma entrada está associado a um valor ou classe/rótulo `alvo` mensurado/fixado, que queremos ser capazes de predizer a partir de um novo vetor de *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos dividir o aprendizado supervisionado de acordo com a natureza da variável alvo:\n",
    "\n",
    "* Classificação (variável dependente categórica): processo de identificação da classe-alvo a partir dos *features*\n",
    "* Regressão (variável dependente quantitativa): cálculo de um número a partir das variáveis de entrada, estando presente alguma noção de métrica/distância/similaridade entre os possíveis alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendo do contexto, há outros nomes possíveis para essa variáveis ([Dependent and independent variables na Wikipedia](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)):\n",
    "\n",
    "> [...], an independent variable is sometimes called a \"predictor variable\", \"regressor\", \"controlled variable\", \"manipulated variable\", \"explanatory variable\", \"exposure variable\" (see reliability theory), \"risk factor\" (see medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable\"\n",
    "\n",
    "> [...], a dependent variable is sometimes called a \"response variable\", \"regressand\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Datasets no scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os datasets que podem ser abertos diretamente com o scikit-learn são dicionários com pelo menos os seguintes valores associados às seguintes chaves:\n",
    "\n",
    "* `DESCR`: Descrição, em texto, do conteúdo do dataset\n",
    "* `data`: `np.ndarray` $2D$ com os *features* ou variáveis independentes (entrada), em que cada coluna é um *feature*\n",
    "* `target`: `np.ndarray` $1D$ com os alvos ou variáveis dependentes (saída)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando aplicável, há ainda:\n",
    "\n",
    "* `feature_names`: nomes dos *features* (colunas do `data`)\n",
    "* `target_names`: rótulos/*labels* para cada valor possível de `target`\n",
    "* `images`: `np.ndarray` com a imagem de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos abrir um dataset famoso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict = datasets.load_iris()\n",
    "iris_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: type(v) for k, v in iris_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dict[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dict[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída categórica com índices para esses nomes\n",
    "iris_dict[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(iris_dict[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(iris_dict[\"data\"], columns=iris_dict[\"feature_names\"]) \\\n",
    "         .assign(species=[iris_dict[\"target_names\"][idx] for idx in iris_dict[\"target\"]])\n",
    "iris.iloc[[0, 50, 100], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriam esses dados linearmente separáveis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há uma classe/espécie que é linearmente separável das demais! Você conseguiria colocar alguma reta acima em algum dos gráficos para separar uma distribuição das outras duas?\n",
    "\n",
    "As outras duas espécies criam um bloco só, parecem mais difíceis de separar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse dataset, na versão atual, contém erros de transcrição, felizmente irrelevantes para este tutorial. Veja o artigo *The use of multiple measurements in taxonomic problems* (1933) do R. A. Fisher e a análise em https://github.com/danilobellini/scientific-literature para mais informações sobre o Iris, discriminantes lineares e o fundamento estatístico de parte do que será visto a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scikit-learn, há um padrão para todos os **estimadores**, objetos que são treinados para estimar/predizer valores. No caso da classificação, fazemos isso com os métodos `fit` e `predict`, um para treinar o classificador, o outro para utilizá-lo.\n",
    "\n",
    "```python\n",
    "from sklearn import módulo_com_classificador\n",
    "estimator = módulo_com_classificador.ClasseDoClassificador(*args, **kwargs)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "```\n",
    "\n",
    "Cada entrada pode ser tanto um *array* do NumPy como um *dataframe* do Pandas:\n",
    "\n",
    "* `X_train` e `X_test` são $2D$ e possuem um evento por linha, uma feature por coluna, como as matrizes do campo `data` dos datasets que vêm com o scikit-learn\n",
    "* `y_train` é $1D$ e possui o resultado de cada respectivo evento, i.e., `y_train[i]` é o resultado do vetor de features `X_train[i]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: Os nomes `X_train` e `X_test` não seguem a [PEP8](https://www.python.org/dev/peps/pep-0008/). Eles decorrem do uso tradicional de letras maiúsculas para representar matrizes e letras minúsculas para representar vetores, em matemática. A variável `X` é tradicionalmente utilizada para as matrizes de \"entrada\" e a `y` para o vetor (matriz coluna) de \"saída\". Porém, isso é apenas uma convenção de nomes, o contexto pode sugerir nomes mais descritivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue uma lista não-exaustiva dos classificadores disponíveis no scikit-learn:\n",
    "\n",
    "* `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`\n",
    "* `sklearn.ensemble.AdaBoostClassifier`\n",
    "* `sklearn.ensemble.RandomForestClassifier`\n",
    "* `sklearn.gaussian_process.GaussianProcessClassifier`\n",
    "* `sklearn.linear_model.LogisticRegressionCV`\n",
    "* `sklearn.linear_model.Perceptron` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.naive_bayes.GaussianNB` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.neighbors.KNeighborsClassifier`\n",
    "* `sklearn.neural_network.MLPClassifier` $\\rightarrow$ possui `partial_fit`\n",
    "* `sklearn.svm.LinearSVC`\n",
    "* `sklearn.svm.SVC`\n",
    "* `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível identificar a espécie de Iris a partir das $4$ medidas (largura e comprimento das sépalas e pétalas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.iloc[:, :4]\n",
    "y = iris[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm # SVM = Support Vector Machine\n",
    "svm_estimator = svm.SVC(kernel=\"linear\") # Classificador linear\n",
    "svm_estimator.fit(X, y) # Treino!\n",
    "svm_estimator.predict(X) # Parece com 50 setosa, 50 versicolor, 50 virginica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a proporção de acertos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_estimator.score(X, y) # Realiza o predict internamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando temos um classificador, como podemos avaliá-lo?\n",
    "\n",
    "Poderíamos usar os dados que temos para realizar essa avaliação, porém se esses dados são os mesmos utilizados para treinar o estimador, talvez estejamos propagando um vício sem saber, obtendo um resultado demasiado otimista devido à falta de variabilidade. O nome dado a esse resultado é **overfitting**.\n",
    "\n",
    "Para auxiliar o entendimento do conceito, podemos fazer uma analogia disso com a aplicação de uma prova depois de uma aula. Durante a aula, o aluno pode encontrar um padrão como \"*as respostas desse tipo de exercício formam uma P.A.*\", por isso ter ocorrido nos exemplos mostrados, e achar que essa regra generalizava o que foi ensinado. Mas, se o padrão encontrado era meramente contingencial, esse aluno poderá se sair mal na prova por responder com os detalhes irrelevantes que memorizou ao invés de tentar aprender o conteúdo.\n",
    "\n",
    "Por outro lado, em uma aula o professor explicita o que é esse conteúdo, enquanto que em generalizações realizadas a partir de dados, qualquer padrão, mesmo que não seja considerado o mais simples por algum critério, pode ser o conteúdo, e o que queremos testar é se o padrão encontrado faz sentido como generalização dos dados e dos eventos futuros. Uma forma de testar isso é reservando uma parte dos dados que temos para validar/testar a generalização realizada a partir dos demais dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo o dataset em 2 partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma prática comum em *machine learning* é a de dividir os dados fornecidos em $2$ conjuntos: um para a realização do treinamento (`fit`) do estimador, outro para a realização de uma avaliação do estimador obtido. Um conjunto para o \"treino\", outro para \"testes\". Entre os diversos modelos, adota-se o que minimiza o erro olhando apenas para o conjunto de testes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](overfitting.png)](https://en.wikipedia.org/wiki/File:Overfitting_svg.svg)\n",
    "\n",
    "(Imagem obtida no Wikipedia)\n",
    "\n",
    "As curvas do gráfico acima representam a taxa de erro quando avaliamos o modelo com o conjunto de testes (vermelho) ou com o conjunto de treino (azul). O eixo das abscissas representa os modelos, por exemplo os resultados de cada iteração de um processo de treinamento. Continuar um treinamento pode nos levar a uma contínua melhora na acurácia do modelo quando olhamos somente para o conjunto de treinamento, mas em algum instante o resultado aplicado ao conjunto de testes indica que o modelo começou a perder acurácia: overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y,\n",
    "                                     test_size=.25,\n",
    "                                     random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso separou $25\\%$ dos dados para teste e $75\\%$ para treino. Esse é o *default* do scikit-learn, o argumento nominado `test_size` não precisava ter sido passado nesse caso, mas é útil explicitar esse número, e torna prático mudá-lo, caso necessário.\n",
    "\n",
    "Poderíamos fazer esse particionamento manualmente, mas isso já particionou nossos dados embaralhando-os, sem perder a correspondência de índices entre as respectivas entradas e saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chamar o `fit` novamente sobreescreve os dados, apagando o treino anterior!\n",
    "# Classificadores que permitem aprendizado incremental possuem um método `partial_fit`!\n",
    "svm_estimator.fit(X_train, y_train)\n",
    "y_pred = svm_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos avaliar se esse resultado `y_pred` é similar ao `y_test`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos realizar avaliações sumarizando os erros e acertos de cada natureza em uma matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linhas representam os resultados coletados, as colunas representam os resultados estimados. Cuidado para não se confundir: isso é o transposto do [exposto no Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix), mas é a forma descrita na documentação do scikit-learn.\n",
    "\n",
    "O scikit-learn sempre ordena os dados ao apresentá-los, de maneira que as 3 linhas e as 3 colunas referem-se às espécies *setosa*, *versicolor* e *virginica* (ou 0, 1 e 2, se estivéssemos usando o `datasets.load_iris()[\"target\"]`), nesta ordem. Para fixar uma ordem qualquer, usa-se o argumento nominado `labels`.\n",
    "\n",
    "Colocando rótulos com o Pandas para melhorar a visualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_pred, labels=labels),\n",
    "             index=[[\"y_test\"] * 3, labels],\n",
    "             columns=[[\"y_pred\"] * 3, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que fizemos até o momento pode ser re-escrito da seguinte forma, sem utilizar o Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection, svm, metrics\n",
    "\n",
    "iris_dict = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(iris_dict[\"data\"], iris_dict[\"target\"],\n",
    "                                     test_size=.25, random_state=0)\n",
    "\n",
    "# Extra: o \"fit\" devolve o próprio objeto estimador\n",
    "svm_estimator = svm.SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "y_pred = svm_estimator.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_pred) # Implicitamente labels=[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem fixar o `random_state`, o resultado talvez não seja o mesmo, pois chamar `train_test_split` re-embaralha os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando as técnicas vistas, treinar um classificador para separar os seguintes blobs gerados aleatoriamente, avaliando o resultado obtido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_blob, y_blob = datasets.make_blobs(n_samples=2000, n_features=2, centers=4, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.FacetGrid(pd.DataFrame(X_blob).assign(y=y_blob), hue=\"y\", aspect=1, size=7) \\\n",
    "       .map(plt.scatter, 0, 1, s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Combinando blocos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o que foi feito até agora, é suficiente saber que o classificador `sklearn.svm.SVC(kernel=\"linear\")` e o `sklearn.svm.LinearSVC` encontram um discriminante linear para separar as classes, maximizando a distância com relação às amostras (margem larga), a depender do valor do `C` (quanto maior, mais tenderá à margem máxima).\n",
    "\n",
    "Porém, um discriminante linear é um hiperplano no espaço de *features* que consegue apenas separar duas classes (espécies, no caso do Iris dataset). Para funcionar com múltiplas classes, o `sklearn.svm.SVC` resolve isso agindo como um \"one-on-one classifier\". O que é isso? Tem alternativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas classes são *decorators* (no sentido do *design pattern*) para classificadores binários:\n",
    "\n",
    "* `sklearn.multiclass.OneVsOneClassifier`: Classifica todos os pares de classes separadamente, selecionando o que tiver o maior número de votos\n",
    "* `sklearn.multiclass.OneVsRestClassifier`: Cria um classificador para cada classe, avaliando se pertence à mesma ou ao conjunto de todas as demais classes\n",
    "\n",
    "No caso do SVM, se quiséssemos usá-lo como *one-vs-rest*, teríamos de instanciar o estimador desta maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import multiclass\n",
    "one_vs_rest_svm_estimator = multiclass.OneVsRestClassifier(svm.SVC(kernel=\"linear\"))\n",
    "one_vs_rest_svm_estimator.fit(X_train, y_train)\n",
    "one_vs_rest_svm_estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos acesso aos classificadores binários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vs_rest_svm_estimator.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `decision_function` do estimador (classificador) binário é usado como critério de desempate, ou a primeira coluna do `predict_proba`, que representa a probabilidade da classe \"positiva\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No caso do SVM, estas são as distâncias com relação ao\n",
    "# hiperplano que separa as classes\n",
    "one_vs_rest_svm_estimator.estimators_[0].decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quando não é um classificador binário, cada coluna do\n",
    "# `decision_function` refere-se a um classificador binário\n",
    "one_vs_rest_svm_estimator.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais informações sobre classificadores multi-classe, veja http://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendo do estimador, um ajuste aditivo/multiplicativo para normalização da entrada pode ser fundamental para treiná-lo, influenciando o comportamento do mesmo. O fluxo do pré-processamento é similar ao de classificação, dessa vez com os métodos `fit` e `transform`:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "pre_processor = preprocessing.ClasseDePréProcessamento(*args, **kwargs)\n",
    "pre_processor.fit(X_train) # Encontra os parâmetros para pré-processar\n",
    "X_data_p = pre_processor.transform(X_data) # Processa um conjunto de dados\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando aplicado nos dados de treino, esse passo de transformação/pré-processamento passa a ser parte do pipeline de classificação, sendo necessário aplicá-lo nos dados antes do `predict` do classificador:\n",
    "\n",
    "```python\n",
    "X_train_p = pre_processor.transform(X_train)\n",
    "X_test_p = pre_processor.transform(X_test)\n",
    "\n",
    "estimator.fit(X_train_p, y_train) # Classificador do dado pré-processado!\n",
    "y_pred = estimator.predict(X_test_p) # Predição também exige dado pré-processado!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes comuns para pré-processar colunas/*features* independentemente incluem:\n",
    "\n",
    "* `StandardScaler`: subtrai a média e divide pelo desvio padrão\n",
    "* `RobustScaler`: subtrai a mediana e divide pelo IQR\n",
    "* `MinMaxScaler`: transformação afim do intervalo `[mínimo, máximo]` dos dados para o intervalo `[0, 1]`\n",
    "* `MaxAbsScaler`: transformação afim do intervalo `[-m, m]` para o intervalo `[-1, 1]`, em que `m` é o máximo do valor absoluto do *feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "iris_scaler = preprocessing.MinMaxScaler()\n",
    "iris_scaler.fit(X_train)\n",
    "X_train_p = iris_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_measurements = [cname.rsplit(None, 1)[0] for cname in iris_dict[\"feature_names\"]]\n",
    "iris_p = pd.DataFrame(X_train_p, columns=iris_measurements) \\\n",
    "           .assign(species=[iris_dict[\"target_names\"][idx] for idx in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_p, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exceção, a classe `sklearn.preprocessing.Normalizer` realiza um pré-processamento em cada linha, isoladamente, para esta ter norma unitária. Por não ter estado, o método `fit` de tal classe não faz nada, porém ele existe para que essa classe possa fazer parte de uma pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p_normalized = preprocessing.Normalizer().transform(X_train_p)\n",
    "iris_p_normalized = pd.DataFrame(X_train_p_normalized, columns=iris_measurements) \\\n",
    "                      .assign(species=[iris_dict[\"target_names\"][idx] for idx in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_p_normalized, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um pouco burocrático ter de realizar o pré-processamento manualmente antes do classificador:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing, módulo_com_classificador\n",
    "\n",
    "pre_processor = preprocessing.ClasseDePréProcessamento(*args, **kwargs)\n",
    "pre_processor.fit(X_train)\n",
    "\n",
    "X_train_p = pre_processor.transform(X_train)\n",
    "classifier = módulo_com_classificador.ClasseDoClassificador(*args, **kwargs)\n",
    "classifier.fit(X_train_p, y_train)\n",
    "\n",
    "X_test_p = pre_processor.transform(X_test)\n",
    "y_pred = classifier.predict(X_test_p)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, queremos um objeto classificador que abstraia o processo inteiro de pré-processamento e de classificação, isto é, um classificador que já realiza o pré-processamento, para não ser necessário pré-processar cada entrada antes de usar o classificador. A realidade é que isso existe! O `sklearn.pipeline.Pipeline` conecta blocos de pré-processamento (objetos com `fit` e `transform`) a um único classificador recebendo uma lista de pares:\n",
    "\n",
    "```python\n",
    "from sklearn import pipeline\n",
    "pipeline_classifier = pipeline.Pipeline([\n",
    "    (\"Nome da etapa 1 de pré-processamento\", objeto_pre_processador1),\n",
    "    (\"Nome da etapa 2 de pré-processamento\", objeto_pre_processador2),\n",
    "    # ...\n",
    "    (\"Nome da etapa N de pré-processamento\", objeto_pre_processadorN),\n",
    "    (\"Nome da etapa de classificação\", objeto_classificador)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "pipeline.Pipeline([(\"normaliza\", preprocessing.StandardScaler()),\n",
    "                   (\"classifica\", svm.LinearSVC())]) \\\n",
    "        .fit(X_train, y_train) \\\n",
    "        .score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: OCR de números!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn possui um dataset de dígitos escritos à mão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits_dict = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar os primeiros 10 dígitos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.subplots(2, 5, figsize=(15, 6), sharex=True, sharey=True)[1]\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits_dict[\"images\"][idx], cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parte 1*: Bisbilhotar/fuçar/investigar esse dataset. Nesse passo é preciso, no mínimo, identificar o tamanho de cada imagem, o número total de imagens e a quantidade de cada dígito presente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parte 2*: Utilizando algum dos classificadores disponíveis com o scikit-learn e as técnicas já vistas, criar um OCR para dígitos a partir desse dataset, reservando $30\\%$ do mesmo para a realização de uma avaliação exposta na forma de uma matriz de erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3 - Aprendizado não-supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora estudamos apenas o processo de classificação supervisionada, no qual tínhamos um rótulo categórico para cada vetor de *features*, e gostaríamos de predizê-lo quando novos dados surgissem. Se tivéssemos apenas os *features*, poderíamos inferir alguma informação a partir dos mesmos? Sim, e esses são os processos de aprendizado não-supervisionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering com o k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente o algoritmo mais famoso de clustering é o k-means. Ele está implementado no scikit-learn na classe `sklearn.cluster.KMeans`. Como classificador, ele possui os métodos `fit` e `predict`, mas dessa vez o `fit` recebe apenas uma matriz de features para treinamento, e os labels são apenas números de $0$ a $k - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O k-means realiza um ciclo para a obtenção de \"centróides\":\n",
    "\n",
    "1. Define $k$ pontos como centróides\n",
    "2. Classifica cada ponto (linha da matriz de entrada) com o centróide mais próximo do mesmo (distância euclideana, tipicamente)\n",
    "3. Atualiza os centróides com os novos centróides de cada conjunto classificado\n",
    "\n",
    "A repetição dos passos $2$ e $3$ até a convergência é o que caracteriza o algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](K-means_convergence.gif)](https://en.wikipedia.org/wiki/File:K-means_convergence.gif)\n",
    "\n",
    "(Imagem obtida no Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificar na prática, vamos aplicar o k-means aos blobs gerados anteriormente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O primeiro parâmetro é o número de clusters\n",
    "kmeans_model = cluster.KMeans(4, random_state=42)\n",
    "kmeans_model.fit(X_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_blob_pred = kmeans_model.predict(X_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Mesmo sem conhecer as classes de cada amostra, o algoritmo encontrou $4$ clusters a partir apenas dos pares de coordenadas de cada ponto. Vamos comparar visualmente o resultado desse processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob_dframe = pd.DataFrame(X_blob)\n",
    "full_blob_dframe = pd.concat([\n",
    "    X_blob_dframe.assign(y=y_blob, origin=\"Original\"),\n",
    "    X_blob_dframe.assign(y=y_blob_pred, origin=\"K-Means\")\n",
    "])\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.FacetGrid(full_blob_dframe, hue=\"y\", col=\"origin\",\n",
    "                  aspect=1, size=5, sharey=True) \\\n",
    "       .map(plt.scatter, 0, 1, s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecem bastante próximos! As cores diferentes apenas representam que o índice arbitrário de cada cluster não são iguais. Podemos fingir que esse processamento foi supervisionado e calcular uma matriz de erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casamento/reordenação das classes realizada empiricamente de forma a\n",
    "# fazer a diagonal da matriz ter os maiores valores de cada coluna\n",
    "metrics.confusion_matrix(y_blob, np.array([3, 0, 1, 2])[y_blob_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas $18$ dos $2000$ pontos usados para gerar os blobs não coincidiram. Mesmo sendo uma técnica de aprendizado não-supervisionado, pode-se dizer que ela foi eficaz em identificar as classes nesse caso artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar o k-means ao Iris dataset e verificar o resultado através de uma matriz de erro. O k-means consegue classificar corretamente as $3$ espécies? E se usássemos outro valor para o $k$, como $2$, $4$ ou $6$, o que acontece?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação de fontes de áudio com ICA - Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado um arquivo de áudio stereo, por exemplo:\n",
    "\n",
    "https://freesound.org/s/380165/\n",
    "\n",
    "Podemos separar os lados esquerdo e direito do áudio, convertendo em dois arquivos WAVE, `left.wav` e `right.wav`, usando o FFmpeg:\n",
    "\n",
    "```shell\n",
    "ffmpeg -i arquivo_baixado.mp3 -map_channel 0.0.0 left.wav -map_channel 0.0.1 right.wav\n",
    "```\n",
    "\n",
    "Os lados esquerdo e direito são muito similares nesse arquivo de áudio, mas seria possível separar as duas fontes sonoras (os $2$ cantores), ou pelo menos criar dois arquivos de áudio em que algum dos cantores está em maior evidência que o outro?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar encontrar componentes independentes no áudio usando o `sklearn.decomposition.FastICA`.\n",
    "\n",
    "Para ler os arquivos de áudio como um *array* do NumPy e armazenar depois os resultados em arquivos WAVE, vamos utilizar o módulo `scipy.io.wavfile` do SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "rate, left = wavfile.read(\"left.wav\")\n",
    "right = wavfile.read(\"right.wav\")[1]\n",
    "rate # Amostras de áudio por segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input = np.stack([left, right]).T\n",
    "audio_input.shape # Par (amostras por canal, número de canais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_input.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O FastICA no scikit-learn é um algoritmo de decomposição, o qual se comporta como uma rotina de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "ica = decomposition.FastICA(n_components=2, random_state=42)\n",
    "ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como outros modelos de pré-processamento, há um método `fit_transform` que unifica os dois passos `fit` e `transform` já vistos, e pode ser mais eficiente caso o processo do `fit` já exija o cálculo do resultado esperado pelo `transform`. A expressão:\n",
    "\n",
    "```python\n",
    "processor.fit_transform(X)\n",
    "```\n",
    "\n",
    "é funcionalmente equivalente a:\n",
    "\n",
    "```python\n",
    "processor.fit(X).transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = preprocessing.MaxAbsScaler().fit_transform(ica.fit_transform(audio_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wavfile.write(\"result_0.wav\", rate, output[:, 0])\n",
    "wavfile.write(\"result_1.wav\", rate, output[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados desse processamento estão disponíveis nos links:\n",
    "\n",
    "* `result_0.wav`: https://freesound.org/s/406216/\n",
    "* `result_1.wav`: https://freesound.org/s/406215/\n",
    "\n",
    "As vozes não estão totalmente separadas, mas, por exemplo, a partir do instante 1:37 (últimos 20 segundos) é possível notar que cada áudio destaca mais uma das vozes, distintamente, sem distorcê-las.\n",
    "\n",
    "É possível obter resultados melhores usando técnicas específicas para essa finalidade, mas é importante notar que esse processo de segregação em componentes independentes não utilizou nada além do arquivo de áudio stereo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para maiores detalhes sobre ICA no scikit-learn, vejam estes dois exemplos na documentação do scikit-learn:\n",
    "\n",
    "* http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html\n",
    "* http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html\n",
    "\n",
    "O primeiro link contém, em particular, uma decomposição de sinais sintéticos, o qual ilustra bem o processo de decomposição realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte Final - O aprendizado não acaba aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muitas coisas importantes que não puderam ser apresentadas neste tutorial introdutório, desde questionamentos gerais de ciência de dados:\n",
    "\n",
    "* Como coletar os dados, e como esse processo pode influenciar os processos que seguem?\n",
    "* Como identificar quais dados são relevantes para um estimador?\n",
    "* Como tratar dados incompletos, espúrios e outliers?\n",
    "\n",
    "Como questionamentos mais diretamente vinculados ao scikit-learn:\n",
    "\n",
    "* Como selecinar o algoritmo de classificação?\n",
    "* Como podemos fazer para encontrar o valor ideal do `k`?\n",
    "* Como encontrar os melhores coeficientes do SVM?\n",
    "* Como tratar *features* categóricos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte desse conteúdo exige conhecimento não apenas do scikit-learn, mas com outras bibliotecas. Por exemplo, para tratar features categóricos pode-se usar o `sklearn.preprocessing.OneHotEncoder`, mas talvez o `pd.get_dummies` do Pandas seja mais adequado para o que se deseja fazer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Minha sugestão para o autodidata que deseja continuar aprendendo sobre scikit-learn é que procure por material sobre:\n",
    "\n",
    "* Regressão: Os modelos de regressão são similares aos vistos para a classificação, com a diferença de que o resultado não é mais categórico\n",
    "* PCA: Decomposição fundamentada nas dimensões de maior variabilidade comumente utilizada para redução da dimensionalidade\n",
    "* Grid Search: busca exaustiva por parâmetros para os estimadores\n",
    "\n",
    "E veja os exemplos disponibilizados em http://scikit-learn.org/stable/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FIM! Espero que tenha gostado! =D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
